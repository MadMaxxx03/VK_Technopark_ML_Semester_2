{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5867f95-05e4-4fbe-8d61-3f34a4cc1354",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "d76dbc89-88a2-4be9-bd25-5b2ec0ef64a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee4c6d-97d5-4b8f-9141-308f4ffed7c4",
   "metadata": {},
   "source": [
    "## Считывание и преобразование данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5111ffae-1ee4-4ffe-9c08-c30e8f51c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),   \n",
    "    transforms.ToTensor(),           \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                         std=[0.5, 0.5, 0.5])  \n",
    "])\n",
    "\n",
    "data = ImageFolder(\"data/\", transform=transform)\n",
    "\"\"\"\n",
    "class_indices = {i: np.where(np.array(data.targets) == i)[0] for i in range(len(data.classes))}\n",
    "subset_indices = []\n",
    "\n",
    "for indices in class_indices.values():\n",
    "    subset_size = int(0.5 * len(indices))\n",
    "    selected_indices = np.random.choice(indices, subset_size, replace=False)\n",
    "    subset_indices.extend(selected_indices)\n",
    "\n",
    "subset_data = Subset(data, subset_indices)\n",
    "\n",
    "train_size = int(0.8 * len(subset_data))\n",
    "val_size = int(0.1 * len(subset_data))\n",
    "test_size = len(subset_data) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(subset_data, [train_size, val_size, test_size])\n",
    "\"\"\"\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = int(0.1 * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(data, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338987b4-3e0a-4939-8fe6-ebea3c374b49",
   "metadata": {},
   "source": [
    "## SummaryWriter для сохранения результататов эксперементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "e4792fb0-9aee-45b2-b1f8-687a88149b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboardWriter = SummaryWriter('logs/experiment_14')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96902e00-bb28-4f5b-b324-5664177015e9",
   "metadata": {},
   "source": [
    "## Структура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "84067b77-46b6-4788-a534-ba9fbcdbb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 8 * 8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49f6c1-b9c6-4fe2-8101-afca3483d1c9",
   "metadata": {},
   "source": [
    "## Проверка доступности видеокарты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3ff3ba29-a038-4ecc-bade-9013fad5ccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available()) \n",
    "print(torch.cuda.device_count())   \n",
    "print(torch.cuda.current_device()) \n",
    "print(torch.cuda.get_device_name(0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a8aca6-7428-424a-80b8-fce36475cec7",
   "metadata": {},
   "source": [
    "## Перевод расчетов модели на видеокарту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5c06f7ee-5fc8-4f69-9097-3f9ceac151f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8a9a87d4-30bf-468d-aea2-66f8201a7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename='checkpoint.pth'):\n",
    "    \"\"\"Сохранение состояния модели\"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, start_epoch=0):\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(start_epoch, start_epoch + num_epochs)):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "        \n",
    "        tensorboardWriter.add_scalar('Loss/train', train_loss, epoch)\n",
    "        tensorboardWriter.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "        tensorboardWriter.add_scalar('Loss/val', val_loss, epoch)\n",
    "        tensorboardWriter.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "        \n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f'Validation accuracy improved from {best_val_acc:.4f} to {val_acc:.4f}. Saving model...')\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': val_loss,\n",
    "                'accuracy': val_acc\n",
    "            }\n",
    "            save_checkpoint(checkpoint, 'cnn_best_checkpoint.pth')\n",
    "            \n",
    "    \n",
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    loss = running_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a1726-3d86-4cf9-9b1b-fc8985a00cd9",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9c7cbc59-554f-4a51-89bc-efa6e42a21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████████▌                                                                     | 1/7 [14:49<1:28:57, 889.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/7], Loss: 0.2240, Accuracy: 0.9153, Val Loss: 0.2344, Val Accuracy: 0.9128\n",
      "Validation accuracy improved from 0.0000 to 0.9128. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████████████████▏                                                         | 2/7 [29:40<1:14:10, 890.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/7], Loss: 0.2193, Accuracy: 0.9165, Val Loss: 0.2206, Val Accuracy: 0.9163\n",
      "Validation accuracy improved from 0.9128 to 0.9163. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████▌                                               | 3/7 [44:26<59:14, 888.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/7], Loss: 0.2168, Accuracy: 0.9177, Val Loss: 0.2240, Val Accuracy: 0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|███████████████████████████████████████████████▍                                   | 4/7 [59:12<44:22, 887.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/7], Loss: 0.2199, Accuracy: 0.9191, Val Loss: 0.2306, Val Accuracy: 0.9145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|█████████████████████████████████████████████████████████▊                       | 5/7 [1:13:44<29:23, 881.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/7], Loss: 0.2120, Accuracy: 0.9200, Val Loss: 0.2190, Val Accuracy: 0.9190\n",
      "Validation accuracy improved from 0.9163 to 0.9190. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|█████████████████████████████████████████████████████████████████████▍           | 6/7 [1:28:25<14:41, 881.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/7], Loss: 0.2114, Accuracy: 0.9202, Val Loss: 0.2271, Val Accuracy: 0.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 7/7 [1:43:13<00:00, 884.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/7], Loss: 0.2091, Accuracy: 0.9214, Val Loss: 0.2203, Val Accuracy: 0.9178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a256e-6e8d-453b-b5bd-7d53df335646",
   "metadata": {},
   "source": [
    "## Результат точности предсказний на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8212f10a-773a-4acb-a513-8f17e5e04f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2307, Test Accuracy: 0.9151\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2f5d6f77-a619-41df-aa57-822f23ae5a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Как в семинаре Test Loss: 1.2679, Test Accuracy: 0.8753\n",
    "# 2 Как в семинаре, но l_rate = 0.0004 Test Loss: 0.9375, Test Accuracy: 0.8622\n",
    "# 3 Добавил новый слой в полносвязанную часть (1024, 128) Test Loss: 1.1222, Test Accuracy: 0.8612\n",
    "# 1, 2, 3 - на 10% данных\n",
    "# 4 Как в предыдущем случае но на 50% данных (слишком долго)\n",
    "# 5 20% данных без 3 слоя свертки Test Loss: 0.9078, Test Accuracy: 0.8648\n",
    "# 6 20% данных 4 слоя свертки 5 этох Test Loss: 0.2779, Test Accuracy: 0.8953\n",
    "# 7 10% данных 4 слоя свертки 5 этох Test Loss: 0.3122, Test Accuracy: 0.8721\n",
    "# 8 20% данных первая архитектура 5 этох Test Loss: 0.0008, Test Accuracy: 1.0000\n",
    "# 9 25% даных 4 свертки 2 полных Test Loss: 0.2804, Test Accuracy: 0.8887\n",
    "# 10 25% даных 4 свертки c 16 до 128 2 полных слоя 5 эпох Test Loss: 0.2849, Test Accuracy: 0.8918\n",
    "# 11 25% даных 4 свертки c 16 до 128 2 полных слоя 7 эпох Test Loss: 0.3030, Test Accuracy: 0.8925\n",
    "# 12 25% даных 4 свертки c 16 до 128 2 полных слоя 5 эпох Test Loss: 0.5584, Test Accuracy: 0.8896\n",
    "# 13 50% даных 4 свертки c 16 до 128 2 полных слоя 7 эпох Test Loss: 0.2544, Test Accuracy: 0.9005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff27e0b-f601-4844-9807-bb3548e1aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 100% данных 4 сертки с 16 до 128 2 полных слоя 4 эпохи Test Loss: 0.2399, Test Accuracy: 0.9043"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae539f9-0f48-417d-9a86-982f121b9d18",
   "metadata": {},
   "source": [
    "## Загрузка модели и дообучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "144ffc9c-66c1-4970-9f2f-06faf426380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filename, model, optimizer):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss'] \n",
    "    accuracy = checkpoint['accuracy']\n",
    "    print(f\"Checkpoint loaded. Epoch: {epoch},  Val Loss: {loss}, Val Accuracy: {accuracy}\")\n",
    "    return model, optimizer, epoch, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "95cdf56a-f07c-4d58-be01-e9e6d4dc6799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baben_bakg1j1\\AppData\\Local\\Temp\\ipykernel_19688\\894992187.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Epoch: 5,  Val Loss: 0.2189508285562182, Val Accuracy: 0.9190051641820131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 1/5 [14:34<58:19, 874.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/5], Loss: 0.2103, Accuracy: 0.9206, Val Loss: 0.2265, Val Accuracy: 0.9159\n",
      "Validation accuracy improved from 0.0000 to 0.9159. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 2/5 [29:21<44:04, 881.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/5], Loss: 0.2097, Accuracy: 0.9206, Val Loss: 0.2643, Val Accuracy: 0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [44:03<29:23, 881.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5], Loss: 0.2079, Accuracy: 0.9213, Val Loss: 0.2292, Val Accuracy: 0.9167\n",
      "Validation accuracy improved from 0.9159 to 0.9167. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [58:51<14:44, 884.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/5], Loss: 0.2125, Accuracy: 0.9205, Val Loss: 0.2419, Val Accuracy: 0.9141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 5/5 [1:35:50<00:00, 1150.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/5], Loss: 0.2051, Accuracy: 0.9222, Val Loss: 0.2549, Val Accuracy: 0.9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model, optimizer, start_epoch, val_loss, best_val_acc = load_checkpoint('cnn_best_checkpoint.pth', model, optimizer)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5, start_epoch=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8efa6202-657f-4582-ae3a-b133d2cfe78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2626, Test Accuracy: 0.9047\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92067937-46d7-4334-87a7-cd54135ec282",
   "metadata": {},
   "source": [
    "## Визуализация логов эксперементов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "dd1071e3-e424-4007-b4fb-cf4ab9af0433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-577b451f0b7177d7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-577b451f0b7177d7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_dir = 'logs/'\n",
    "notebook.start(\"--logdir \" + log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602a52c8-871a-4520-a9a3-bfc0570ea091",
   "metadata": {},
   "source": [
    "## Наилучший результат (Accuracy ~ 0.916)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "59875cba-4275-4061-820c-7c63ecb109bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Epoch: 10,  Val Loss: 0.2292349541810975, Val Accuracy: 0.9167397447140212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baben_bakg1j1\\AppData\\Local\\Temp\\ipykernel_19688\\894992187.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model, optimizer, start_epoch, val_loss, best_val_acc = load_checkpoint('cnn_best_checkpoint.pth', model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "281b968f-179e-40c8-b070-834c55b53a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2417, Test Accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099caab-796b-407d-a689-870406910855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–¥–∞–Ω–∏–µ: –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±—É–∫–≤ –¥–ª—è –∑–∞–¥–∞—á–∏ —Ä–∞—Å—Å—Ç–∞–Ω–æ–≤–∫–∏ —É–¥–∞—Ä–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–æ–≤ –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ transformers. –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –≤–∑—è—Ç—å –æ—Ç—Å—é–¥–∞: https://github.com/Koziev/NLP_Datasets/blob/master/Stress/all_accents.zip\n",
    "\n",
    "1. –ù–∞–ø–∏—à–∏—Ç–µ –∫–ª–∞—Å—Å –¥–ª—è Dataset/Dataloder –∏ —Ä–∞–∑–±–µ–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã–µ train / test —Å–ø–ª–∏—Ç—ã –≤ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–∏ 50:50. (1 –±–∞–ª–ª)\n",
    "2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±—É—á–∏—Ç—å –æ–¥–Ω—É –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑ –º–æ–¥–µ–ª–µ–π: Bert, Albert, Deberta. –ü–æ—Å—á–∏—Ç–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫—É Accuracy –Ω–∞ train –∏ test. (1 –±–∞–ª–ª). –ü—Ä–∏ –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–∞ –≤ Accuracy –Ω–∞ test 0.8: (+1 –±–∞–ª–ª), 0.85: (+2 –±–∞–ª–ª–∞), 0.89: (+3 –±–∞–ª–ª–∞).\n",
    "–ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è deberta: https://huggingface.co/IlyaGusev/ru-word-stress-transformer/blob/main/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5FaCG9ajnS_G"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778eca178c0c45c0809d91021866c4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baben_bakg1j1\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\baben_bakg1j1\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027be841cd9947f78f97017b01c81894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 39, 42, 26, 12, 18, 46, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import sys\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, Trainer, AutoModelForTokenClassification, TrainingArguments, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from character_tokenizer import CharacterTokenizer\n",
    "\n",
    "chars = \"–ê–∞–ë–±–í–≤–ì–≥–î–¥–ï–µ–Å—ë–ñ–∂–ó–∑–ò–∏–ô–π–ö–∫–õ–ª–ú–º–ù–Ω–û–æ–ü–ø–†—Ä–°—Å–¢—Ç–£—É–§—Ñ–•—Ö–¶—Ü–ß—á–®—à–©—â–™—ä–´—ã–¨—å–≠—ç–Æ—é–Ø—è\"\n",
    "model_max_length = 64\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length)\n",
    "\n",
    "data = pd.read_csv('all_accents.tsv', sep='\\t', header=None, names=[\"word\", \"stressed_word\"])\n",
    "#model = AutoModelForTokenClassification.from_pretrained(\"IlyaGusev/ru-word-stress-transformer\") \n",
    "\n",
    "#model_name = \"DeepPavlov/rubert-base-cased\"  \n",
    "#model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2) \n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#model_name = \"microsoft/deberta-base\"  # DeBERTa v2 –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model=AutoModelForTokenClassification.from_pretrained('bert-base-uncased')\n",
    "#tokenizer=AutoTokenizer.from_pretrained(\"KoichiYasuoka/bert-base-russian-upos\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "example = \"–ü—Ä–∏–≤–µ—Ç\"\n",
    "tokens = tokenizer(example)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\n",
      "       word stressed_word\n",
      "0      -–¥–µ          -–¥^–µ\n",
      "1      -–∫–∞          -–∫^–∞\n",
      "2    -–ª–∏–±–æ        -–ª^–∏–±–æ\n",
      "3  -–Ω–∏–±—É–¥—å      -–Ω–∏–±^—É–¥—å\n",
      "4       -—Å            -—Å\n",
      "–ü—Ä–∏–º–µ—Ä –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:\n",
      "–°–ª–æ–≤–æ: —Å—É–±—Å–∏–¥–∏—Ä—É—é—â–∏–µ\n",
      "–ü–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:\\n\", data.head())\n",
    "\n",
    "def get_stress_position(word, stressed_word):\n",
    "    return stressed_word.index(\"^\") - 1 if \"^\" in stressed_word else None\n",
    "\n",
    "data['stress_position'] = data.apply(lambda row: get_stress_position(row['word'], row['stressed_word']), axis=1)\n",
    "data = data.dropna(subset=['stress_position'])\n",
    "data['stress_position'] = data['stress_position'].astype(int)\n",
    "\n",
    "data = data.sample(n=50000, random_state=42)\n",
    "\n",
    "train_words, test_words, train_labels, test_labels = train_test_split(\n",
    "    data['word'].tolist(), data['stress_position'].tolist(), test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –∏–∑ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏:\")\n",
    "print(\"–°–ª–æ–≤–æ:\", train_words[2])\n",
    "print(\"–ü–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è:\", train_labels[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StressDataset(Dataset):\n",
    "    def __init__(self, words, labels, tokenizer, max_length=42):\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        tokens = self.tokenizer(word, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        \n",
    "        label_ids = [-100] * len(input_ids)\n",
    "        if 1 <= label < len(input_ids) - 1:\n",
    "            label_ids[label + 1] = 1  \n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(tokens[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = StressDataset(train_words, train_labels, tokenizer)\n",
    "test_dataset = StressDataset(test_words, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    mask = labels != -100\n",
    "    labels = labels[mask]\n",
    "    preds = preds[mask]\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baben_bakg1j1\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\baben_bakg1j1\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7815' max='7815' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7815/7815 12:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=None \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {eval_results['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–ª–æ–≤–æ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞:\n",
      "–°–ª–æ–≤–æ: –Ω–µ–æ—Ç–≤—è–∑—á–∏–≤–æ–º\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 4\n",
      "\n",
      "–°–ª–æ–≤–æ: —Å–±—Ä–∏–≤–∞–ª—Å—è\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 4\n",
      "\n",
      "–°–ª–æ–≤–æ: –∏—Å—Ç–∞–∏–≤–∞—é—â–∏—Ö\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 2\n",
      "\n",
      "–°–ª–æ–≤–æ: –ø—Ä–æ—Ç–æ–ø–∏–≤—à–∏–µ\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 5\n",
      "\n",
      "–°–ª–æ–≤–æ: –≤—ã–±–∏–≤–∞–µ–º–∞—è\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 4\n",
      "\n",
      "–°–ª–æ–≤–æ: –ø–µ—Å—Ç—Ä—è–¥–∏–Ω–Ω–æ–º—É\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 6\n",
      "\n",
      "–°–ª–æ–≤–æ: —Ç–µ–∑–µ–µ\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 2\n",
      "\n",
      "–°–ª–æ–≤–æ: —Ä–∞—Å—Ç–∞—Å–∫–∏–≤–∞–≤—à–∏–π\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 3\n",
      "\n",
      "–°–ª–æ–≤–æ: –∑–∞—Ä–∞–∑–∏–≤—à–∏—Ö—Å—è\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 4\n",
      "\n",
      "–°–ª–æ–≤–æ: —Ç—Ä–µ–≤–æ–∂–Ω–µ–π—à–∏–µ\n",
      "  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: -1\n",
      "  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "random_indices = random.sample(range(len(test_dataset)), 10)\n",
    "\n",
    "print(\"–°–ª–æ–≤–æ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ç–∫–∞:\")\n",
    "for idx in random_indices:\n",
    "    word = test_words[idx]\n",
    "    \n",
    "    true_label = test_labels[idx]\n",
    "    pred_label = None  \n",
    "\n",
    "    for i, label in enumerate(predicted_labels[idx]):\n",
    "        if label == 1:\n",
    "            pred_label = i - 1  \n",
    "            break\n",
    "\n",
    "    print(f\"–°–ª–æ–≤–æ: {word}\")\n",
    "    print(f\"  –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: {pred_label}\")\n",
    "    print(f\"  –†–µ–∞–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è —É–¥–∞—Ä–µ–Ω–∏—è: {true_label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–ª–æ–≤–æ: –¥–µ–∫–ª–∞–º–∞—Ü–∏—é\n",
      "–¢–æ–∫–µ–Ω—ã: [0, 16, 18, 30, 32, 8, 34, 8, 54, 26, 70, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "–ú–µ—Ç–∫–∏: [-100, -100, -100, -100, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: –¥–µ–∫–ª–∞–º–∞—Ü–∏—é\n",
      "\n",
      "–°–ª–æ–≤–æ: –∑–∞–ø—É—Ç—ã–≤–∞–ª–∞—Å—å\n",
      "–¢–æ–∫–µ–Ω—ã: [0, 24, 8, 40, 48, 46, 64, 12, 8, 32, 8, 44, 66, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "–ú–µ—Ç–∫–∏: [-100, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: –∑–∞–ø—É—Ç—ã–≤–∞–ª–∞—Å—å\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_example(word, stress_position):\n",
    "    tokens = tokenizer(word, padding=\"max_length\", truncation=True, max_length=42)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    \n",
    "    label_ids = [-100] * len(input_ids)\n",
    "    if 1 <= stress_position < len(input_ids) - 1:\n",
    "        label_ids[stress_position + 1] = 1  \n",
    "    \n",
    "    decoded_word = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"–°–ª–æ–≤–æ: {word}\")\n",
    "    print(f\"–¢–æ–∫–µ–Ω—ã: {tokens['input_ids']}\")\n",
    "    print(f\"–ú–µ—Ç–∫–∏: {label_ids}\")\n",
    "    print(f\"–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ: {decoded_word}\")\n",
    "    print()\n",
    "\n",
    "check_example(\"–¥–µ–∫–ª–∞–º–∞—Ü–∏—é\", 5)  \n",
    "check_example(\"–∑–∞–ø—É—Ç—ã–≤–∞–ª–∞—Å—å\", 2)  \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
